##################################
## Function 1: QR decomposition ##
##################################

myQR <- function(A){
  
  ## Perform QR decomposition on the matrix A
  ## Input: 
  ## A, an n x m matrix
  
  n = dim(A)[1]
  m = dim(A)[2]
  R <- A
  Q <- diag(n)
  
  for(k in 1:(m-1)){
    x <- R[k:n,k]
    v <- x
    v[1] <- x[1] + sign(x[1]) * sqrt(sum(x^2))
    u <- v/sqrt(sum(v^2))
    H <- diag(n)
    H[k:n,k:n] <- diag(n-k+1) - 2 * u %*% t(u)
    R <- H %*% R
    Q <- Q %*% H
    
  }
  
  ## Function should output a list with Q.transpose and R
  ## Q is an orthogonal n x n matrix
  ## R is an upper triangular n x m matrix
  ## Q and R satisfy the equation: A = Q %*% R
  
  return(list("Q" = t(Q), "R" = R))
  
}

###############################################
## Function 2: Linear regression based on QR ##
###############################################

myLinearRegression <- function(X, Y){
  
  ## Perform the linear regression of Y on X
  ## Input: 
  ## X is an n x p matrix of explanatory variables
  ## Y is an n dimensional vector of responses
  ## Do NOT simulate data in this function. n and p
  ## should be determined by X.
  ## Use myQR inside of this function
  
  n = dim(X)[1]
  p = dim(X)[2]+1
  
  intercept <- rep(1,n)
  A <- cbind(intercept,X,Y)
  output <- myQR(A)
  R <- output$R[1:p,]
  beta_hat <- rep(0,p)
  
  for(i in p:1){
    beta_hat[i] <- (R[i,p+1] - R[i,1:p] %*% beta_hat) / R[i,i] 
  }
  
  error <- sqrt(sum(output$R[(p+1):n,]^2))
  ## Function returns the 1 x (p + 1) vector beta_ls, 
  ## the least squares solution vector
  
  return(list(beta_hat=beta_hat, error=error))
  
}

testing <- function(){

  ## This function is not graded; you can use it to
  ## test out the 'myLinearRegression' function

  ## Define parameters
  n<- 100
  p <- 3

  ## Simulate data from our assumed model.
  ## We can assume that the true intercept is 0
  X <- matrix(rnorm(n * p), nrow = n)
  beta <- matrix(1:p, nrow = p)
  Y<- X %*% beta + rnorm(n)

  ## Save R's linear regression coefficients
  R_coef <- coef(lm(Y ~ X))
  print(R_coef)

  ## Save our linear regression coefficients
  my_coef <- myLinearRegression(X, Y)
  print(my_coef$beta_hat)

  ## Are these two vectors different?
  sum_square_diff <- sum((R_coef - my_coef$beta_hat)^2)
  if(sum_square_diff <= 0.001){
    return('Both results are identical')
  }else{
    return('There seems to be a problem...')
  }

}
